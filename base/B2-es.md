# elasticsearch

关键是熟悉读写操作和了解脑裂。

## 写数据

1. 选择一个node作为协调节点发过去
2. 协调节点对数据（doc）进行路由，将请求转发给对应的node
3. 转发后的node的主分片才是真正处理请求的，然后把数据同步到所有的副本节点
4. 当主节点和副本节点都完成后，协调节点才返回结果。

底层：

1. doc先写入内存buffer，同时写translog日志

2. refresh：创建、打开和写入一个新段，插入一个追加的倒排索引。

3. 每一秒钟会从buffer创建一个新的段，这里的新段会先写入文件系统缓存。稍后再刷新到磁盘。**只要文件已经在缓存中就可以被打开和读取了**

4. 内存buffer清空，这时候新段的文件就可以被搜索了
    https://www.elastic.co/guide/cn/elasticsearch/guide/current/near-real-time.html

5. 还需要做flush：目的是持久化数据，它会提交一个commit并且截断translog。refresh，缓存会被清空但是事务日志不会，所以translog就会越来越多，**当translog日志大于一个阈值或者生命周期大于30mins，就会触发flush操作**。

    1. 所有在内存缓冲区的doc写入到一个新的段
    2. 清空缓冲
    3. 一个提交点写入硬盘，表明有哪些段被commit了
    4. 文件系统缓存通过fsync到硬盘
    5. 删除老的translog

    一般是每30mins做一次flush，也可以手动使用_flush命令

6. 此外，translog每五秒会写入磁盘，**如果这五秒的时候数据在缓冲区buffer而且log没有持久化就会丢失此次log**

总结是有三个批次操作，一秒做一次refresh保证近实时搜索，5秒做一次translog持久化保证数据未持久化前留底，30分钟做一次数据持久化。

## 读数据

一般通过doc id进行查询

1. 客户点发送请求到协调节点
2. 协调节点根据id进行哈希，把请求转发到应该去的node
    **此处使用robin随机轮询算法**，将会在主分片和所有副本中随机选择一个，目的是负载均衡
3. 接受请求的node返回doc给协调节点
4. 协调节点返回doc到客户端

## 搜索数据

比如搜索某个doc是否包含abc

1. 客户点发送请求到协调节点
2. 协调节点把搜索请求转发到所有分片对应的主分片和副本分片
3. 进入query phase，每个副本会把自己的搜索结果（这时候一般是id）返回协调节点
4. 由协调节点将进行合并、排序、分页等操作
5. fetch phase：由协调节点根据id去每个节点上拉取实际的doc
6. 返回给客户端

## 倒排索引

在搜索引擎之中每个doc对应一个文档id，

经过分词，文档内容表示为一系列关键词的集合。

每个关键词都会记录他在文档中出现的次数和位置，

**倒排索引就是关键词到文档id的映射，每个关键词都对应着一系列的文件**

实际中使用倒排索引，还可以记录文档的频率信息。

## 实际坑

### 建立超大索引

建立超大索引的时候，一定要先关闭自动刷新，相当于直接全写入buffer，先不refresh直到写完才refresh。

目的：为了防止新段过多，因为当相似大小的新段过多，es内部会不断把这种新段进行合并，并且物理删除原段。

### 查询巨量doc

不可以采用from-size法，这会使得es内部产生深度分页，而且当from+size>10000的时候就无法查询了。

需要使用search-after法，当然缺点是无法O1跳转到指定页数。

### 脑裂

出现了多个master，并且造成查询不同机器，查到了结果不同的情况。

成因：it在调网络没有通知我们，造成集群之间网络延迟超大，以为master挂掉了就选出了新master。还有因为data节点上面的es进程占用太高，引发JVM大规模内存回收，造成ES进程失去响应。

解决：

1. 增加了响应时间，原本是3s，目前调到了10s。
2. 把选举通过数量改为 (n/2)+1，n是主节点个数
3. 把master和data节点分离，限制角色，这样master挂了data进程还能响应，还能再撑一会。

最终：增加一台物理机，手动分析数据，调整参数，之后先启动数据最先进的节点使其成为master。