#    操作系统

![](https://ling_boogie.gitee.io/peasonal_pic/cpp_base/os1.png)

## ⭐page cache

目的：**加快从磁盘读取文件的速度**

page cache中会有一部分文件的缓存，因为从磁盘中读取文件慢，所以读取文件会先去page cache中查找

命中，直接读就可以了

在Linux中，每个数据块最多只能对应一个page cache，通过两个数据结构来维护.

一个是radix tree，另一个是双向链表。

Linux内核利用这个数据结构，快速查找脏的(dirty) 和回写的（writeback）页面，得到**其文件内偏移**，从而对page cache进行快速定位。

树中所有叶子节点为一个个页帧结构（struct page），表示了用于缓存该文件的每一个页。	

在叶子层最左端的第一个页保存着该文件的前4096个字节（如果页的大小为4096字节），接下来的页保存着文件第二个4096个字节，依次类推。

**Radix tree 是一种以Bitmap为思想的多叉搜索树，内核利用这个数据结构来通过文件内偏移快速定位Cache 项**

链接：https://blog.csdn.net/frank_zyp/article/details/98068871

---

## ⭐伙伴系统

伙伴系统是内核中用来管理**物理内存**的一种方法。

目的是缓解物理内存的碎片化

大致设计：把系统要管理的物理内存按照页面个数分为不同的组，确切的说分为了11个组，分别对应11个大小不同的连续内存块，每组中都存在若干个大小相等的内存块（2的幂次），那么系统中就存在2^0^-2^10^ 这十一种大小不同的内存快，而内核就用十一个链表管理这十一个内存块组。

当分配内存的时候

1. 先寻找对应2的幂次大小的组别中的内存块
    1. 有，分配。
    2. 没有，从更大一级的内存块上分配一块内存，并分成一半给我们使用，**剩余的一半挂在对应大小的内存块链表上（核心）**。

```c
static inline
struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
                        int migratetype)
{
    unsigned int current_order;
    struct free_area *area;
    struct page *page;

    /* Find a page of the appropriate size in the preferred list */
    for (current_order = order; current_order < MAX_ORDER; ++current_order) {
        area = &(zone->free_area[current_order]);
        if (list_empty(&area->free_list[migratetype]))
            continue;

        page = list_entry(area->free_list[migratetype].next,
                            struct page, lru);
        list_del(&page->lru);
        rmv_page_order(page);
        area->nr_free--;
        expand(zone, page, order, current_order, area, migratetype);
        set_freepage_migratetype(page, migratetype);
        return page;
    }

    return NULL;
}

```

从底层源码来看，order是不能超过MAX_ORDER=11的，超过了此值不会进入轮询，而是直接返回NULL了。

**SGI-STL设计的二级配置器也是同样的设计思路**

---

## 用户态和内核态的区别

最大的区别就是特权级别不同

用户态拥有最低的特权级，内核态拥有较高的特权级。

运行在用户态的程序不能直接访问操作系统内核数据结构和程序。

**内核态和用户态之间的转换方式主要包括：系统调用，异常和中断。**

另外，

- 用户态下的普通进程只能访问 0-3GB 的用户空间；
  - 内核态下的普通进程既能访问 0-3GB 的用户空间，也能访问 3-4GB 的内核空间（内核态下的普通进程有时也会需要访问用户空间）。

----

## ⭐用户态和内核态的转化

### 方式

1. 系统调用，用户进程主动要求切换到内核态。而系统调用的核心是使用了一个操作系统专门给用户开放的中断
   **比如linux的ine 80h中断**
2. 异常，比如缺页异常。
3. 外围设备中断，比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序执行后续操作等。

### 切换操作

1. 从当前进程描述符中提取其内核栈的 **ss0、esp0**信息
2. 使用 **ss0、esp0** 指向的内核栈将当前进程的 **cs、eip、ss、esp**等信息保存起来
3. 将先前由中断向量检索得到的中断处理程序的 **cs、eip** 信息装入对应的寄存器，开始执行中断处理程序，这时候就转到了内核态的程序了。

---

## ⭐进程通信方式

1. 信号（signl）用于[多进程](https://baike.baidu.com/item/多进程)之间的同步

2. 信号量 一个计数器。信号量用于实现进程间的**互斥与同步**，而不是用于存储进程间通信数据。

3. 消息队列 消息的链表，存放在内核中并由消息队列标识符标识。用户进程可以向消息队列添加消息，也可以向消息队列读取消息。可以把消息看做一个记录，具有特定的格式以及特定的优先级。对消息队列有写权限的进程可以向消息队列中**按照一定的规则添加新消息**，对消息队列有读权限的进程可以从消息队列中读取消息。**需要在内核和用户空间进行四次的数据拷贝**，磁盘文件-用户空间-内核空间-用户空间-磁盘文件。

    1. 不再局限于父子进程．而允许**任意进程通过共享消息队列来实现进程间通信**．

    2. 由系统调用函数来实现消息发送和接收之间的同步．从而使得用户在使用消息缓冲进行通信时不再需要考虑同步问题．使用方便。

    3. **信息的复制需要额外消耗CPU的时间**．不适宜于信息量大或操作频繁的场合。

    4. 容量受到系统限制，且要注意第一次读的时候，要考虑上一次没有读完数据的问题

    5. ```cpp
            msgqid=msgget(SVKEY,0777);  //打开 75#消息队列
            pid=getpid();  //获取client进程标识符
            pint=(int *)msg.mtext;  //把正文的内容传给 pint，并强制转换类型
            *pint=pid;  //pint指针指向client进程标识符
            msg.mtype=1;  //消息类型为 1
            msgsnd(msgqid,&msg,sizeof(int),0);  //发送消息msg入msgqid消息队列
            msgrcv(msgqid,&msg,250,pid,0);  //从队列msgqid接收消息msg 
        ```

4. 无名管道
   1. 简单方便．
   2. 局限于单向通信的工作方式
   3. 只能在**具有亲缘关系的进程之间使用**

5. 有名管道
   1. 可以提供给任意关系的进程使用．
   2. **长期存在于系统之中，使用不当容易出错。**
   3. 开启一个管道对操作系统来说是很奢侈的一件事情

6. 共享内存
   配合信号灯实现消息共享与同步

   1. 针对消息缓冲的缺点改而利用内存缓冲区直接交换信息，**无须复制**，快捷、信息量大
   2. 但是共享内存的通信方式是通过将共享的内存缓冲区直接附加到进程的虚拟地址空间中来实现的．因此，这些进程之间的**读写操作的同步问题操作系统无法实现。**必须由各进程利用其他同步工具解决。
   3. 由于内存实体存在于计算机系统中．所以只能由处于同一个计算机系统中的诸进程共享。**不方便网络通信。**

### 套接字

1. 命名socket
   SOCK_STREAM 式本地套接字的通信双方均需要具有本地地址，其中服务器端的本地地址需要明确指定，**指定方法是使用 struct sockaddr_un 类型的变量。**

2. 绑定
   将相应字段赋值，再将其绑定在创建的服务器套接字上，**绑定要使用 bind 系统调用。**

3. 监听
   服务器端套接字创建完毕并赋予本地地址值后，需要进行监听，**等待客户端连接并处理请求，监听使用 listen 系统调用，接受客户端连接使用accept系统调用。**

4. 连接服务器：客户端套接字创建完毕并赋予本地地址值后，需要连接到服务器端进行通信，让服务器端为其提供处理服务。

   **对于SOCK_STREAM类型的流式套接字，需要客户端与服务器之间进行连接才能使用。连接要使用 connect 系统调用。**

5. 相互发送接受数据
   无论客户端还是服务器，都要和对方进行数据上的交互，这种交互也正是我们进程通信的主题。一个进程扮演客户端的角色，另外一个进程扮演服务器的角色，两个进程之间相互发送接收数据，这就是基于本地套接字的进程通信。**发送和接收数据要使用 write 和 read 系统调用。**

6. 断开连接
   交互完成后，需要将连接断开以节省资源，**使用close系统调用。**

----

## 进程构造

在内核中用 <code>task_struct</code>描述，这也就是 **狭义的PCB**。

切换进程肯定发生中断，因为**系统需要从目态切换到管态** 。

切换到管态是因为需要使用处理机调度程序，而此程序在操作系统底层，**需要高级别特权。**

- 标识符：描述本进程的唯一标识符，int，有限
- 任务状态：退出状态、退出信号等等
- 优先级：用于调度
- 程序计数器PC：下一条指令的地址
- **内存指针**：包括程序代码和进程相关数据的指针，还有和其他进程共享的内存区的指针
- **上下文数据**：进程执行的时候处理器寄存器的数据
- I/O状态信息：包括显式的io请求，分配给当前进程的io设备，被进程所使用的文件列表
- 记账信息：可能包括处理器时间总和，使用的时钟数总和，时间限制等等

保存上下文

1. 用户级上下文：正文（运行处），数据，用户堆栈，共享存储区
2. 寄存器上下文/硬件上下文：通用寄存器，指针指令寄存器IP，处理器状态寄存器EFLAGS，栈顶指针ESP
   1. 尽管每个进程可以拥有属于自己的地址空间，但所有进程必须共享CPU寄存器。因此，在恢复一个进程的执行之前，内核必须确保每个寄存器装入了挂起进程时的值。
   2. **在Linux中，进程硬件上下文的一部分存放在任务状态（TSS）段，而剩余部分存放在内核态堆栈中。**
3. 系统级上下文：进程控制块task_struct，内存管理块（mm_struct，vm_area_struct，堆栈）
   1. task_struct 中的 mm_struct 对象用于管理该进程（或者线程共享的）页表。准确地说，mm_struct 中的 pgd 指针指向着该进程的页全局目录。

0222：在中台项目中，曾对湿度传感器开发板改进过entrt.S处的某个寄存器地址，需要备注，不一定是16个通用。

-----

## 线程构造（没有上下文）

需要保存当前**线程Id、线程状态、堆栈、寄存器状态**等信息。

在内核中就是一个类似PCB的TCB，但是内容少很多

**TCB：**

- 标识符：唯一表示线程，long

- 线程运行状态

- 优先级

- 堆栈指针

- 一组寄存器：IP寄存器等

- 线程专有的存储区（局部存储区TLS）
    <code>static __thread int buf[MAX_ERROR_LEN];</code>
    **\_\_thread变量并不是在线程之间完全隐藏的，每个线程保存自己的一份拷贝**，每个线程的这个变量的地址不同，但是如果一个线程获得了另一个线程的局部变量的地址那么也可以对其进行存取。

    注意：TLS的数据只能是first class，或者是Plain IO'data
    更高级的局部：专属数据。需要使用线程库实现。

    - 在posix库中使用getthreadspecific和setthreadspecific 组件来实现这一特性，编译要加-pthread
    - 创建一个键（key），用以将不同的线程特有数据区分开来。**调用函数pthread_key_create()可创建一个key**，且只需要在首个调用该函数的线程中创建一次。
    - 在不同线程中，**使用pthread_setspecific()函数将这个key和本线程（调用者线程）中的某个变量的值关联起来**，这样就可以做到不同线程使用相同的key保存不同的value。
    - 在各线程可**通过pthread_getspecific()函数来取得本线程中key对应的值**，各个线程同一key所保存的变量不尽相同。

- 信号屏蔽

**寄存器切换：** 

1.  ESP:堆栈指针，指向当前栈的栈顶地址 
2.  PC:程序计数器，存储下一条将要执行的指令 
3.  EAX:累加寄存器，用于加法乘法的缺省寄存器 

---

## 内核线程

现代操作系统将一些重要的任务，如刷新磁盘高速缓存，交换出不用的页框，维护网络连接等，委托给内核线程，内核线程不受不必要的用户态上下文拖累。

Linux中，内核线程在以下几方面不同于普通进程：

1. 内核线程只运行在内核态，而普通进程既可以运行在内核态，也可以运行在用户态
2. 因为内核线程只运行在内核态，所以他只使用PAGE_OFFSET的线性地址空间；而普通进程不管是在用户态还是内核态，都可以使用4GB的线性地址空间
   1. PAGE_OFFSET 代表的是内核空间和用户空间对虚拟地址空间的划分，对不同的体系结构不同。比如在32位系统中3G-4G 属于内核使用的内存空间，所以 PAGE_OFFSET =  0xC0000000。在X86-64架构下是ffff880000000000。可以看到内核程序可以可以访问从PAGE_OFFSET 之后的内存，访问所有的信息（注意页的写保护）。 
3. 其进程描述符内的mm指针被设置为NULL
   1. 大多数计算机上系统的全部虚拟地址空间分为两个部分: 供用户态程序访问的虚拟地址空间 mm 和供内核访问的内核空间。
   2. 对于普通用户进程来说，mm指向虚拟地址空间的用户空间部分，而对于内核线程，mm为NULL。
   3. 这为优化提供了一些余地, 可遵循所谓的惰性TLB处理(lazy TLB handing)。
   4. active_mm主要用于优化，由于内核线程不与任何特定的用户层进程相关，内核并不需要倒换虚拟地址空间的用户层部分，保留旧设置即可。由于内核线程之前可能是任何用户层进程在执行，故用户空间部分的内容本质上是随机的，内核线程决不能修改其内容，故将mm设置为NULL，同时如果切换出去的是用户进程，内核将原来进程的mm存放在新内核线程的active_mm中，因为某些时候内核必须知道用户空间当前包含了什么。

> 为什么没有mm指针的进程称为惰性TLB进程?
>
> 假如内核线程之后运行的进程与之前是同一个, 在这种情况下, 内核并不需要修改用户空间地址表。地址转换后备缓冲器(即TLB)中的信息仍然有效。只有在内核线程之后, 执行的进程是与此前不同的用户层进程时, 才需要切换(并对应清除TLB数据)。

kernel_thread()函数创建一个新的内核线程，它接受的参数有：所要执行的内核函数的地址、要传递给函数的参数、一组clone标志

**但该函数本质上调用do_fork()，并且只能由其他内核线程创建**

注意，在内核其实没有线程的概念，内核把所有线程都当成进程来实现，并没有准备特别的调度算法和数据结构来表示线程。

内核线程是直接由内核本身启动的进程。内核线程实际上是将内核函数委托给独立的进程，它与内核中的其他进程”并行”执行。内核线程经常被称之为**内核守护进程**。

他们执行下列任务

1. 周期性地将修改的内存页与页来源块设备同步
2. 如果内存页很少使用，则写入交换区
3. 管理延时动作,　如２号进程接手内核进程的创建
4. 实现文件系统的事务日志

内核线程主要有两种类型：

1. 线程启动后一直等待，直至内核请求线程执行某一特定操作。
2. 线程启动后按周期性间隔运行，检测特定资源的使用，在用量超出或低于预置的限制时采取行动。

内核线程由内核自身生成，其特点在于：

1. 它们在CPU的管态执行，而不是用户态。
2. 它们只可以访问虚拟地址空间的内核部分（高于TASK_SIZE的所有地址），但不能访问用户空间

链接：https://www.cnblogs.com/alantu2018/p/8526916.html

系统在正式启动内核时，会执行 **start_kernel** 函数。在这个函数中，会自动创建一个进程，名为 **init_task**。其 PID 为 0，运行在内核态中。然后开始执行一系列初始化。

###  init 内核线程

init_task 在执行 rest_init 函数时，会执行 kernel_thread 创建 init 内核线程。它的 PID 为 1，用来完成内核空间初始化。

在内核空间完成初始化后，会调用 exceve 执行 init 可执行程序 (/sbin/init)。之后，init 内核线程变成了一个普通的进程，运行在用户空间中。

>   init 内核线程没有地址空间，且它的 task_struct 对象中的 mm 为 NULL。因此，执行 exceve 会使这个 mm 指向一个 mm_struct，而不会影响到 init_task 进程的地址空间。
>   也正因为此，init 在转变为进程后，其 PID 没变，仍为 1。

创建完 init 内核线程后，init_task 进程演变为 idle 进程（PID 仍为 0）。

之后，init 进程再根据再启动其它系统进程 (/etc/init.d 目录下的各个可执行文件)。

###  kthreadd 内核线程

init_task 进程演变为 idle 进程后，idle 进程会执行 kernel_thread 来创建 kthreadd 内核线程（仍然在 rest_init 函数中）。它的 PID 为 2，用来创建并管理其它内核线程（用 kthread_create, kthread_run,  kthread_stop 等内核函数）。

系统中有很多内核守护进程 (线程)，可以通过：
`ps -efj`
进行查看，其中带有 [] 号的就属于内核守护进程。它们的祖先都是这个 kthreadd 内核线程。

### 主内核页全局目录

内核维持着一组自己使用的页表，也即主内核页全局目录。当内核在初始化完成后，其存放在 swapper_pg_dir 中，而且所有的普通进程和内核线程就不再使用它了。

### 内核线程如何访问页表

#### active_mm

对于内核线程，虽然它的 task_struct 中的 mm 为 NULL，但是它仍然需要访问内核空间，因此需要知道关于内核空间映射到物理内存的页表。然而不再使用 swapper_pg_dir，因此只能另外想法解决。

由于所有的普通进程的页全局目录中的后面部分为主内核页全局目录，因此内核线程只需要使用某个普通进程的页全局目录就可以了。

在 Linux 中，task_struct 中还有一个很重要的元素为 active_mm，它主要就是用于内核线程访问主内核页全局目录。

- 对于普通进程来说，task_struct 中的 mm 和 active_mm 指向的是同一片区域；
- 然而对内核线程来说，task_struct 中的 mm 为 NULL，active_mm 指向的是前一个普通进程的 mm_struct 对象。

#### mm_users 和 mm_count

但是这样还是不行，因为如果因为前一个普通进程退出了而导致它的 mm_struct 对象也被释放了，则内核线程就访问不到了。

为此，mm_struct 对象维护了一个计数器 mm_count，专门用来对引用这个 mm_struct  对象的自身及内核线程进行计数。初始时为 1，表示普通进程本身引用了它自己的 mm_struct 对象。只有当这个引用计数为 0  时，才会真正释放这个 mm_struct 对象。

另外，mm_struct 中还定义了一个 mm_users 计数器，它主要是用来对共享地址空间的线程计数。事实上，就是这个主线程所在线程组中线程的总个数。初始时为 1。

>   注意，两者在实质上都是针对引用 mm_struct 对象而设置的计数器。
>   不同的是，mm_count 是专门针对自身及内核线程或引用 mm_struct 而进行计数；而 mm_users 是专门针对该普通线程所在线程组的所有普通线程而进行计数。
>   另外，只有当 mm_count 为 0 时，才会释放 mm_struct 对象，并不会因为 mm_users 为 0 就进行释放。

---

## ⭐进程跟线程

进程是资源分配的最小基本单位，线程是CPU调度的最小基本单位。

### 相同点（简述）

都是用来实现多任务并发的技术手段，都可以被独立调度。

都有个各自的实体，在多任务程序中子进程/子线程一般与父进程/父线程平等竞争。

对于 Linux 来讲，所有的线程都当作进程来实现，因为没有单独为线程定义特定的调度算法，也没有单独为线程定义特定的数据结构（所有的线程或进程的核心数据结构都是 task_struct）。

> 对于一个进程，相当于是它含有一个线程，就是它自身。对于多线程来说，原本的进程称为主线程，它们在一起组成一个线程组。

### 相同点（数据结构）

每个进程或线程都有三个数据结构，分别是 **struct thread_info**, **struct task_struct** 和 **内核栈**。

> 注意，虽然线程与主线程共享地址空间，但是线程也是有自己独立的内核栈的。

thread_info 对象中存放的进程/线程的基本信息

它和这个进程/线程的内核栈存放在内核空间里的一段 2 倍页长的空间中。其中 thread_info 结构存放在低地址段的末尾，其余空间用作内核栈。内核使用 **伙伴系统** 为每个进程/线程分配这块空间。

thread_info 结构体中有一个 **struct task_struct \*task**，task 指向的就是这个进程或线程相关的 task_struct 对象（也在内核空间中）

这个对象叫做进程描述符（叫做任务描述符更为贴切，因为每个线程也都有自己的 task_struct）。内核使用 **slab** 分配器为每个进程/线程分配这块空间。



### 创建方式的区别（使用/表面）

想要获得资源，最起码要有进程。

 多进程中，一个进程的终止并不会影响到其他进程。

多线程中，父线程终止，全部子线程都会被迫终止（没有资源了）。任何一个子线程执行了exit()，全部线程同时灭亡。

进程的实现一般是fork系统调用

```cpp
pid_t fork(void);
```

这个过程将父进程的全部资源复制给了子进程。

进程还可以使用vfork()进行创建，这时候子进程会共享父进程的资源。

多进程一般采用派生+执行的模式，
也就是说fork之后紧接着执行execv启动一个全新的进程，exec启动一个新的程序替换当前的进程PID不变。而**子进程在这时候会返回，父进程就认为子进程“结束”了，父进程开始运行，之间不会干扰**。

现代Unix内核引入了三种不同机制解决进程创建问题：

1、写时复制技术，允许子进程读相同的物理页。只要两者中有一个试图写一个物理页，内核就把这个页的内容拷贝到一个新的物理页，并把这个新的物理页分配给正在写的进程。

2、轻量级进程允许父子进程共享每个进程在内核的很多数据结构

3、vfork()系统调用创建的进程能共享其父进程的内存地址空间。为了防止父进程重写子进程需要的数据，阻塞父进程的执行，一直到子进程退出或执行一个新的程序为止。

**线程的实现是clone系统调用**

```cpp
int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ...);
```

线程的clone只是复制了很小一部分必要的资源。

实际上在创建进程时并不直接使用clone系统调用，而是采用线程库函数。

常用的线程库函数有linux-native线程库和posix线程库。

其中应用最广泛的是posix线程库，因此能经常看到使用pthread_create()。

### 创建方式的区别（本质）

进程拥有自己的地址空间，所以每个进程都有自己的页表。而线程却没有，只能和其它线程共享某一个地址空间和同一份页表。

产生这个区别的根本原因是，**在进程/线程创建时，clone可以有选择性的继承父进程的资源，可以选择是否拷贝当前进程的地址空间还是共享当前进程的地址空间，使得指定的参数不同，最后创建出来的task_struct实体不同**。

具体地说，进程和线程的创建最终都会执行 do_fork 内核函数，而 do_fork 则又会调用 copy_process 内核函数来完成。主要包括如下操作：

- 在调用 copy_process 的过程中，会创建并拷贝当前进程的 task_stuct，同时还会创建属于子进程的 thread_info 结构以及内核栈。
- 此后，会为创建好的 task_stuct 指定一个新的 pid（在 task_struct 结构体中）。
- 
- 然后**根据传递给 clone 的参数标志，来选择拷贝还是共享打开的文件，文件系统信息，信号处理函数，进程地址空间等。这就是进程和线程不一样地方的本质所在**。

### 多道程序设计模式的区别

进程是独立的，所以做到资源独立管理是很容易的。

而且各个进程之间不会互相干扰，也不会有二义性

而子线程与父线程，如果想要做到资源独立，则需要设计必要的线程通信以共享/独占，这样一来会增加开销而且父线程的处理动作不能连贯，效率下降。

但是在有些时候资源不独立会成为优点，线程不需要任何手段就能共享数据，而多进程则需要进程间通信。

当然多个线程在同时写入操作（共享资源）的时候需要实现互斥，否则会发生脏读。



### 通信方式的异同

进程间通信：

1. 无名管道
2. 有名管道
3. 信号量
4. 信号
5. 共享内存
6. 消息队列
7. 套接字
8. 文件

线程间通信

上述进程间的方式除了套接字，几乎都能沿用。

且还有自己独特的几种

1. 互斥量
2. 自旋锁
3. 条件变量
4. 读写锁
5. 线程信号
6. 全局变量

需要注意的是，**线程间通信的信号不能采用进程间的信号，因为信号是基于进程为单位的，而线程是同属于同一进程空间的**，故而要采用线程信号。

**重点**

进程通信需要切换上下文，CPU需要保存当前环境以及设置新环境，要么要与外设访问（有名管道，文件），所以速度比较慢。

而线程通信基本在自己的进程空间内完成，只需要切换ESP、PC、EAX寄存器，通信速度比较快。

### 控制方式的异同

进程与线程的身份标识方式不一样，进程的ID为pid_t类型，实际是一个int变量（有限）

进程的ID是唯一标识，对进程的管理都是通过PID实现的，每创建一个进程，内核都会去创建一个结构体来存储该进程的全部信息。

当子进程结束要回收时，则通过wait()系统调用来进行，未回收的消亡进程会成为僵尸进程，其进程实体已经不存在，但是会虚占PID资源，有可能造成不能再创建新进程的情况。

而线程的ID是一个long变量，范围要比进程ID大得多。

线程ID一般在本进程空间内作用就可以了，当然系统在管理线程的时候也需要记录信息。但是方式很不一样**，内核会创建一个内核态线程与之对应，也就是说每一个用户创建的线程都有一个内核态的线程与之对应。**

但这种对应关系不是一对一的关系，而是多对一的关系，也就是一个内核态线程可以对应多个用户线程。

线程的主动终止需要调用pthread_exit()，主线程需要调用pthread_join()来回收。



### 资源管理方式的异同

进程本身是资源分配的基本单位，因而它的资源都是独立的，如果有多进程间的共享资源，就要用到进程间的通信方式了，比如共享内存。共享数据就放在共享内存去，大家都可以访问，为保证数据写入的安全，加上信号量一同使用。一般而言，共享内存都是和信号量一起使用。消息队列则不同，由于消息的收发是原子操作，因而自动实现了互斥，单独使用就是安全的。

线程间要使用共享资源不需要用共享内存，直接使用全局变量即可，或者malloc()动态申请内存，显得方便直接。而且互斥使用的是同一进程空间内的互斥量，所以效率上也有优势。

实际中**，对于核心数据，进程和线程都采用共享内存来存储。**共享内存脱离进程，就算进程终止，共享内存也可以独立存在而不会回收。

**辈分关系**

对于进程，有着严格的辈分关系，爷爷-父亲-自身-儿子-孙子-...

而对于线程，则只存在一个父线程，其他都是父线程的子线程，都是共享父线程的资源。

### ⭐多进程多线程的选择

https://www.cnblogs.com/virusolf/p/5458325.html

### 进程池和线程池实现的区别

**进程池**

需要预先估计好产生多少进程合适，一般不进行动态延展。

使用一个数组或链表保存进程ID。

让闲置进程pause()挂起，也可以用信号量挂起，还可以用IPC阻塞。

分配任务时，设置共享内存，通过进程间通信以及函数指针，让某个进程去预先指定的地方去读取任务。

最后结束的时候向各个进程发送信号唤醒，改变激活状态让其主动结束，然后逐个wait()即可。



**线程池**

大致思想与进程池一样，但是更加轻量。

要让线程阻塞只需要使用条件变量。

分配任务只需要父线程改变条件就可以激活子线程。

结束时也是可以逐个改变条件并改变激活状态让子线程结束，然后逐个回收即可。

**系统分类**

根据线程和进程的设置，操作系统大致分为如下类型

1. 单进程、单线程，MS-DOS
2. 多进程、单线程，多数UNIX系统都是
3. 多进程、多线程，Win32，Solaris 2.x等等
4. 单进程、多线程， VxWorks

**链接**

https://my.oschina.net/cnyinlinux/blog/367910

https://www.cnblogs.com/fah936861121/articles/8043187.html

------

## 进程状态切换图

![img](https://uploadfiles.nowcoder.com/images/20190313/311436_1552470678794_F9BF116BD97A95A5E655DF9E1672186F)

----

## ⭐如何组织进程

运行队列链表会把 TASK_RUNNING 状态的所有进程组织在一起，但由于对处于暂停、僵死、死亡状态的进程的访问比较简单，**所以Linux并没有为处于TASK_RSTOPPED、EXIT_ZOMBILE或者EXIT_DEAD状态的进程建立专门的链表**。

对于处于 可中断TASK_INTERRUPTIBLE 或 不间断TASK_UNINTERRUPTIBLE 状态的进程，根据不同的特殊事件进行分类并引入等待队列

等待队列表示一组睡眠的进程，当某一条件为真时，由内核唤醒他们

等待队列由双向链表实现。

因为等待队列是由中断处理程序和主要内核函数修改的，**所以必须对这个双向链表提供原子保护以免同时访问**

在这里，同步是通过等待队列头中的lock自旋锁实现的。

如果有两个或多个进程在等待互斥地访问某一资源时，由内核有选择地进行唤醒，而非互斥进程则总是由内核在事件发生时唤醒。

**非互斥进程总是在双向链表的开始位置，互斥进程总是在尾部**

所以内核总是先唤醒非互斥进程然后再唤醒互斥进程。虽然但是，一个等待队列中同时包含互斥进程和非互斥进程的情况是非常罕见的。

---

## ⭐孤儿、僵尸进程

### 孤儿进程

一个父进程退出，但是他fork()的子进程还在运行，那他的子进程就成了孤儿，不过最后会由init进程（进程id为1）收养并且完成状态回收。

### 僵尸进程

**僵尸进程是一个进程必然会经过的过程**

fork了子进程之后，子进程退出，**但是父进程没有调用wait()或者waitpid()来获取子进程的状态信息**，那么子进程的进程描述符依然会保存在系统中。

子进程exit()之后，如果父进程来不及处理，ps命令能看到子进程是'Z'。

如果来得及处理，那就看不到了。

**危害：**

系统进程号是有限的，如果产生大量的僵尸进程就会耗尽

**外部消灭：**

用kill发送 **SIGTERM** 或者 **SIGKILL** 信号消灭产生僵尸进程的进程，那么他的僵尸进程（子进程）就会成为孤儿，顺利被init进程接管。

---

## 如何实现线程池

1. 设置一个生产者消费者队列，把线程作为临界资源
2. 初始化n个线程，并且run
3. 当任务队列空，阻塞所有线程
4. 生产者队列不空，对队列加锁，增加完成后把任务挂在队列上
5. 当消费者队列不空，用条件变量去通知阻塞中的一个线程

---

## 常用线程模型

**Future模型（Java）**

需要结合Callable接口配合使用。 

哲学：把结果放在将来获取，当前主线程并不急于获取处理结果。允许子线程先进行处理一段时间，**处理结束之后就把结果保存下来，当主线程需要使用的时候再向子线程索取。** 

**fork && join模型**

递归拆分任务，回溯合并结果。用来处理一些可以进行拆分的大任务。

其主要是把一个**大任务逐级拆分为多个子任务**，然后分别在子线程中执行，当每个子线程执行结束之后逐级回溯，返回结果进行汇总合并，最终得出想要的结果。 

**reactor模型**

actor在接受到消息之后可以自己进行处理，也可以继续传递（分发）给其它actor进行处理。在使用actor模型的时候需要使用第三方Akka提供的框架。 

**生产者消费者模型**

生产者消费者模型都比较熟悉，其核心是使用一个缓存来保存任务。开启一个/多个线程来生产任务，然后再开启一个/多个线程来从缓存中取出任务进行处理。

**master-worker模型**

master-worker模型类似于任务分发策略，开启一个master线程接收任务，

然后在master中根据任务的具体情况进行分发给其它worker子线程，

然后由子线程处理任务。

如需返回结果，则worker处理结束之后把处理结果返回给master。 

---

## 内存和磁盘 *

存储在磁盘中的程序需要读入到内存后才能运行

**存储方式**
 内存利用电流来实现存储
 磁盘利用磁效应来实现存储的

**存储容量**
 内存是高速高价，而磁盘则是低速廉价

**磁盘缓存**：把从磁盘中读取的数据存储到内存空间中的方式，这样一来，当接下来需要读取同一数据时，就不用通过实际的磁盘，而是从磁盘缓存中把内容读出

**磁盘划分方式**： 划分的方式有扇区方式和可变长方式两种，前者是指将磁盘划分为固定长度的空间，后者则是指把磁盘划分为长度可变的空间

**扇区**：把磁盘表面分成若干个同心圆的空间就是“磁道”，把磁道按照固定大小（能存储的数据长度相同）划分而成的空间就是“扇区”

**簇**：磁盘文件存储管理的最小单位

**簇的大小**： 1 簇可以是 512 字节（1 簇 = 1 扇区）、1KB（1 簇 = 2 扇区）、2KB、4KB、8KB、16KB、32KB（1 簇 = 64 扇区）

**关于簇的容量讨论**：
 1、从存储方式： （尽量减小）不管是硬盘还是软盘，**不同的文件是不能存储在同一个簇中的，否则就会导致只有一方的文件不能被删除**
 2、从读取方式看：（尽量增大）如果减少簇的容量，磁盘访问次数就会增加，就会导致读写文件的时间变长

Q：通过使用内存来提高磁盘访问速度的机制称为什么？

A：Disk Cache （磁盘缓存）
 磁盘缓存是指，把从磁盘中读出的数据存储在内存中，当该数据再次被读取时，不是从磁盘而是直接从内存中高速读出

---

## 顺序读,随机读 *

**磁盘读取时间：**

1. 寻道时间，表示磁头在不同磁道之间移动的时间。
2. 旋转延迟，表示在磁道找到时，中轴带动盘面旋转到合适的扇区开头处。
3. 传输时间，表示盘面继续转动，实际读取数据的时间。

7200转/min，**旋转一周需要8.33ms**
寻道约10ms
所以整个磁盘读取时间在一个磁道上是10ms级的。

**Q：顺序读写和随机读写对于机械硬盘来说为什么性能差异巨大？**

顺序读写适合读取一个大文件
随机读写适合读取多个小文件

1. 顺序读写
    1. **主要时间花费在了传输时间**，而这个时间两种读写可以认为是一样的。
    2. 磁盘会进行预读（局部性原理）
2. 随机读写
    1. **需要多次寻道和旋转延迟。而这个时间可能是传输时间的许多倍。**
    2. 磁盘不会进行预读

-----

## 介绍虚拟内存

**本质**：
 1、通过打时间差的方式，把实际内存的内容和磁盘上的虚拟内存的内容进行部分置换（swap），并同时运行程序

2、虚拟内存虽说是把磁盘作为内存的一部分来使用，但实际上正在运行的程序部分，在这个时间节点上必须存在在内存当中

**目的：**

1. 为了防止不同进程同一时刻在物理内存中运行而对物理内存的争夺的践踏。
2. 扩大程序运行时的“准许内存”

使用了虚拟内存，所有进程共享同一内存。

事实上，**每个进程创建时，内核只是为进程创建了虚拟内存的布局**，具体就是初始化进程控制表中内存相关的链表，实际上并没有把虚拟内存对应的程序数据拷贝到物理内存，**只需要建立好虚拟内存和磁盘文件之间的映射就好了**

当访问到对应位置时，发生缺页异常调入页面来执行程序。

请求分页系统、请求分段系统和请求段页式系统都是针对虚拟内存的，通过请求实现内存与外存的信息置换。 

![](https://ling_boogie.gitee.io/peasonal_pic/cpp_base/os1.png)

### **好处**

- 扩大了地址空间
- 内存保护：每个进程运行在各自的虚拟内存地址空间，互相不能干扰对方。虚存**还对特定的内存地址提供写保护**，可以防止代码或数据被恶意篡改。 
- 进程通信可以直接虚存共享（映射到同一物理地址）
- .当不同的进程使用同样的代码时，比如**库文件中的代码，物理内存中可以只存储一份这样的代码**，不同的进程只需要把自己的虚拟内存映射过去就可以了，节省内存 
- 可以利用碎片空间，只要保证虚拟地址是连续的，而物理地址可以是离散的。

### 缺点

- 管理虚存需要很多额外的数据结构
- 转换地址也需要额外的时间
- 页面的换入换出需要磁盘io，也是很耗时的

## 虚拟地址空间的划分

对于32位系统

每一个普通进程都拥有 4GB 的虚拟地址空间（对于 32 位的 CPU 来说，即 2^32^ B）。

**64位Linux系统的地址空间不是2^32^B， 也不是2^64^B，而一般是2^48^B，也就是256 TB。内核空间占一半（同样是高地址区域），普通进程使用另外一半，也就是 128 TB。**

因为并不需要2^64那么大的寻址空间，过大的空间只会导致资源的浪费。64位linux一般使用48位来表示虚拟地址空间，使用40位来表示物理地址空间，可以通过cat /proc/cpuinfo来查看

主要分为两部分，一部分是用户空间（0-3GB），一部分是内核空间（3-4GB）。每个普通进程都有自己的用户空间，但是内核空间被所有普通进程所共享。

![2.png-41.3kB](http://static.zybuluo.com/cuckootan/geqa4tf7ejc83ejvjzxkcrw4/2.png)

之所以能够使用 3-4GB 的虚拟地址空间（对于普通进程来说），**是因为每个进程的页全局目录中的后面部分存放的是内核页全局目录的所有表项**。

当通过系统调用或者发生异常而陷入内核时，不会切换进程的页表。

此时，处于内核态的普通进程将会直接使用进程页表中前面的页表项即可。**这也是在执行系统调用或者处理异常时没有发生进程的上下文切换的真实原因**。

同样，正因为每个进程的页全局目录中的后面部分存放的是内核页全局目录中的所有表项，**所以所有普通进程共享内核空间。****

另外，

- 用户态下的普通进程只能访问 0-3GB 的用户空间；
- 内核态下的普通进程既能访问 0-3GB 的用户空间，也能访问 3-4GB 的内核空间（内核态下的普通进程有时也会需要访问用户空间）。

----

## PAE物理地址扩展 *

物理地址扩展 (PAE) 是一项处理器功能，它使 x86 处理器能够在支持的 Windows 版本上访问超过 4 GB 的物理内存。在基于 x86 的系统上运行的某些 32 位版本的 Windows Server 可以使用 PAE 访问多达 64 GB 或 128 GB 的物理内存，具体取决于处理器的物理地址大小

Intel通过在处理器上把管脚数从32增加到36，以提高处理器的寻址能力，使其达到2^36=64GB，为此，需引入一种新的分页机制。

PAE、[4GB 调整](https://docs.microsoft.com/en-us/windows/win32/memory/4-gigabyte-tuning)(4GT) 和[地址窗口扩展](https://docs.microsoft.com/en-us/windows/win32/memory/address-windowing-extensions)(AWE) 用于不同的目的，并且可以相互独立使用：

- PAE 允许操作系统访问和使用超过 4 GB 的物理内存。
- 4GT 将进程可用的虚拟地址空间部分从 2 GB 增加到最多 3 GB。
- AWE 是一组 API，它允许进程分配非分页物理内存，然后将该内存的一部分动态映射到进程的虚拟地址空间。

**AWE 不需要 PAE 或 4GT，但通常与 PAE 一起使用以从单个 32 位进程分配超过 4 GB 的物理内存。**

**Intel Itanium 和 x64 处理器体系结构本身可以访问超过 4 GB 的物理内存，因此不提供等效的 PAE。PAE 仅由在基于 x86 的系统上运行的 32 位版本的 Windows 使用。**

使用 PAE，操作系统从两级线性地址转换转变为三级地址转换。

不是将线性地址拆分为三个单独的字段以索引到内存表中，而是将其拆分为四个单独的字段：一个 2 位位域、两个 9 位位域和一个 12 位位域，对应于实现的页面大小由英特尔架构 (4 KB)。

PAE 模式下的页表条目 (PTE) 和页目录条目 (PDE) 的大小从 32 位增加到 64 位。附加位允许操作系统 PTE 或 PDE 引用 4 GB 以上的物理内存。**

**未启用PAE下的4K分页的页表结构**：12个标志位（固定）+ 10位（**指示页表中的索引**）+ 10位（**指示页目录表中的索引**）

**启用PAE下的4K分页的页表结构**：12个标志位（固定）+ 9位（**指示页表中的索引**）+ 9位（**指示页目录表中的索引**）+ 2位（指向新的**4个64位表项**）

此外：

在基于 x64 的系统上运行的 32 位 Windows 中，PAE 还启用了多项高级系统和处理器功能，包括启用硬件的[数据执行保护](https://docs.microsoft.com/en-us/windows/win32/memory/data-execution-prevention)(DEP)、[非均匀内存访问 (NUMA)](https://docs.microsoft.com/en-us/windows/win32/procthread/numa-support)以及将内存添加到系统运行时（热添加内存）。

**PAE 不会更改进程可用的虚拟地址空间量。在 32 位 Windows 中运行的每个进程仍然限于 4 GB 的虚拟地址空间。**

---

## LAAE大地址感知 *

在Windows增加支持/3GB参数以前,一个应用程序是无法访问一个带有高位设置的指针.

一个32位的指针只有前31位地址空间可以被用户模式的应用程序访问.

这剩余的一位不用.

因此有一些聪明的开发者因为其他的目的不愿意在处理内存地址空间时浪费这一位

举例来说:可以用来标志一个指针引用其它应用程序分配的数据类型

这样就/3GB参数就遇到一个难题，因为这种类型的程序不能方便的区分一个合法的指针引用的内存空间在2G的分界线以上还是一个内存地址空间在2G以下，因为它的高位已经被用做它用的指针。

基本上如果一台机器用/3GB的参数启动,这种应用程序是无法运行

为解决这种状况,微软在WIN32的PE文件Characteristics字段增加了一个新的标志位来表示一个程序是否运行在可访问大地址的(Large-Address-Aware Executables)模式.

---

## AWE地址窗口扩展 *

它允许32位[应用软件](https://zh.wikipedia.org/wiki/应用软件)访问超出其虚拟地址空间限制（4GB）的[物理内存](https://zh.wikipedia.org/wiki/随机存取存储器)。最多可支持 64 GB 的物理内存。

**2004年在[Dr. Dobb's Journal](https://zh.wikipedia.org/w/index.php?title=Dr._Dobb's_Journal&action=edit&redlink=1)发布的一篇文章称注意到使用地址窗口扩展分配的内存将不会被写入[页面文件](https://zh.wikipedia.org/wiki/页面文件)，并建议AWE区域因此可作为保护敏感的应用程序数据（如加密[密钥](https://zh.wikipedia.org/wiki/密钥)）的一种方式。**

[[1\]](https://zh.wikipedia.org/wiki/地址窗口扩展#cite_note-1)在地址窗口扩展下将应用程序的虚拟地址空间映射到物理内存的过程被称为“加窗”（windowing），并类似其他环境的[覆盖](https://zh.wikipedia.org/wiki/覆盖_(编程))概念。

问题：

32位的指针是一个整型，只能够存储小于等于0xFFFFFFFF的值，因此只能够引用一个4GB的线性内存地址空间。AWE使应用程序可以突破这个限制，存取所有操作系统支持的内存。

目标：

1. 允许应用程序以一种特殊的方式分配内存，操作系统保证不会将以这种方式分配的内存换出到磁盘
2. 允许应用程序访问比进程地址空间还要多的内存

原理：

在Windows的内存管理功能中，Application MemoryTuning（/3GB）选项可以给私有进程增加50%的地址空间，使用方便，因此成为一种常用方法，但AWE功能更具有弹性和扩展性。

当你为私有进程地址空间增加1GB，这1GB来自kernalmode的地址空间，kernal mode地址空间也由2GB被压缩到1GB。

但这不能算是根本解决方案。

因为对于kernalmode代码，完整2GB的工作空间已经显得狭窄，压缩这部分空间意味着某些内部核心结构也必须要压缩。

**这些结构中主要有机器上用于管理内存的表窗口（tableWindows）。当你将kernalmode部分压缩到1GB后，这个表最大就只能管理16GB的物理内存了。**

如果你的机器拥有64GB物理内存的机器，那你也只能使用16GB。

**AWE使得你可以在可以访问的地址空间提供一块区域（窗口）作为分段传输区，来传输在用户内存空间无法访问的内存地址。**

缺点：

1. Windows限定
2. 应用程序必须具有 Lock Pages in Memory 权限才能使用 AWE。要获得此权限，管理员必须将**内存**中的**锁定页面**添加到用户的**用户权限分配中**。
3. 

-----

## 介绍虚存页面置换算法

比较常见的内存替换算法有：FIFO，LRU，LFU，LRU-K，CLOCK。 

1. FIFO：先进先出
2. LFU：最不经常访问
   1. 开销：排序开销
   2. 缺点：容易颠簸
3. LRU：最近最少使用
   1. 实现：栈式结构，新页面或者命中的页面则将该页面移动到栈底
   2. 缺点：缓存颠簸（可以造数据卡成On）
   3. 缺点：缓存污染
4. LRU-K：最久未使用K次淘汰
   1. 多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到K次的时候，才将数据放入缓存。
5. 改进CLOCK：时钟置换，标志位置换
   1. 每一个访问的页面关联两个**附加位(reference bit)**，标识是否已修改、是否最近使用
   2. 优先换出未修改的（这样就不需要写回磁盘了），其次换出最近未使用的

## ⭐ 中断的同时发生了什么

知乎：中断同时发生时会发生什么？ - Makoto Ruu的回答 - 知乎 https://www.zhihu.com/question/27923521/answer/38714097

TODO：整理流程，但是原文写的太好了，LAZY.....

----

## ⭐介绍缺页中断

缺页中断：在请求分页系统中，可以通过查询页表中的状态位来确定所要访问的页面是否存在于内存中。**每当所要访问的页面不在内存是，会产生一次缺页中断**，此时操作系统会根据页表中的外存地址在外存中找到所缺的一页，将其调入内存。 

**本身是一种中断**

1. 保护CPU现场
2. 分析中断原因
3. 转入缺页中断处理程序处理
4. 恢复CPU现场

**但是和一般中断也有区别**

1. 一条指令执行期间可以有多次缺页中断
2. 缺页中断返回的是：执行产生中断的一条指令
3. 一般中断返回的是：执行下一条指令

---

## 写时复制技术

**Linux采用了写时复制的方法，以减少fork时对父进程空间进程整体复制带来的开销。 **

前提：多个进程要读取它们自己的那部门资源的副本，**那么复制是不必要的。**

解决：只要每个进程保存一个指向该资源的指针就可以了，只有在需要写入的时候才复制一份副本。

因此每个进程都会产生一个幻觉：自己独占那份资源。

写时复制在内核中的实现非常简单。

与内核页相关的数据结构可以被标记为只读和写时复制。

1. 如果有进程试图修改一个页，就会产生一个缺页中断。
2. 内核处理缺页中断的方式就是对该页进行一次透明复制。
3. **清除页面的COW属性**，表示着它不再被共享。 

---

## 介绍页表寻址

页表中的每一项都记录了这个页的基地址。

通过页表，由**逻辑地址的高位部分先找到逻辑地址对应的页基地址**

再由页基地址偏移一定长度就得到最后的物理地址

偏移的长度由逻辑地址的低位部分决定。

之后的，直接不了解（太多了）

补充：在x86_64cpu下Linux有四级页表，支持48位虚拟地址空间。

现在的 Linux 内核中采用四级页表，分别为：

- 页全局目录 (Page Global Directory, pgd)；
- 页上级目录 (Page Upper Directory, pud)；
- 页中间目录 (Page Middle Directory, pmd)；
- 页表 (Page Table, pt)。

task_struct 中的 mm_struct 对象用于管理该进程（或者线程共享的）页表。准确地说，mm_struct 中的 pgd 指针指向着该进程的页全局目录。

-----

## 如何解决死锁

关键：破坏四个必要条件之一

1. 互斥
2. 请求和保持条件：某个进程请求其他进程被拒绝后阻塞，但不会放弃自己已有的资源
3. 不可剥夺：只能自己释放自己的资源
4. 环路等待

具体：

1. 资源一次性分配；破坏2
2. 可剥夺资源：破坏3
3. 资源有序分配：破坏4

-----

## 介绍互斥锁、读写锁

### 互斥锁

mutex，用来保证在任何时刻都只能有一个线程访问该对象。

获取锁操作失败之后，线程会进入睡眠，等待锁释放才唤醒。

### 读写锁

rwlock，分为读锁和写锁。处于读操作的时候可以允许多个线程获得读锁，但是同一时刻只有一个线程能拿到写锁。

而写锁会阻塞其他读写锁。当一个线程在写的时候，读锁完全不能被获取。而且写者优先级高于读者。

### 区别：

读写锁区分读者和写者，互斥锁不区分

互斥锁同一时刻保证只能有一个线程能拿到，而读锁同一时刻内能被多个线程拿到。

---

## 介绍自旋锁

自旋锁：spinlock

但是当获取锁操作失败时，不会进入睡眠，而是会在原地自旋，直到锁被释放。

**优点：**

节省了线程从睡眠到唤醒的开销，在加锁时间短暂的环境下有很好的效率。

**缺点：**

如果是锁时间一般较长的场景，则会很消耗CPU资源

----

## 加法器原理

c-32位系统上做64位加减法：https://wenku.baidu.com/view/797d14020422192e453610661ed9ad51f01d54de.html

来源：https://www.zhihu.com/question/386710054

---

## 大端/小端存储判断

```c
int checkCPU() 
{ 
    union w 
    {  
        int a; 
        char b; 
    } c; 
    c.a = 1; 
    return (c.b == 1); 
} 
```

---

## 微内核和宏内核  *

### 微内核

内核中只有最基本的调度、内存管理。驱动，文件系统等都是用户去实现。

优点：稳定，驱动等错误只会导致相应的守护进程死掉，系统不崩溃

缺点：效率低。典型代表QNX，QNX的文件系统是跑在用户态的进程，称为resmgr的东西，是订阅发布机制，文件系统的错误只会导致这个守护进程挂掉。**不过数据吞吐量就比较不乐观了。** 

### 宏内核

除了最基本的进程、线程管理、内存管理外，将文件系统，驱动，网络协议等等都集成在内核里面，**例如linux内核。** 

优点：效率高

缺点：稳定性差，**开发过程中的bug经常会导致整个系统挂掉**。 

---

## 死循环+连接时新建线程的方法效率有点低，怎么改进？

提前创建好一个线程池，
用生产者消费者模型，创建一个任务队列，队列作为临界资源，
有了新连接，就挂在到任务队列上，队列为空所有线程睡眠。

**改进死循环：使用select epoll这样的IO复用模型**

---

##   windows消息机制 *

当用户有操作(鼠标，键盘等)时，系统会将这些事件转化为消息。

系统为每个打开的进程维护了一个消息队列，系统会将这些消息放到进程的消息队列中，而应用程序会循环从消息队列中取出来消息（轮询），完成对应的操作。

---

## 五种IO模型

1. 阻塞io
2. 非阻塞io
3. 信号驱动io，linux用套接口进行信号驱动IO，**安装一个信号处理函数，进程继续运行并不阻塞，**当IO时间就绪，进程收到 **SIGIO 信号**。然后处理IO事件。
4. IO复用/多路转换IO：linux用select/epoll函数实现IO复用模型，**这两个函数也会使进程阻塞，但是和阻塞IO所不同的是这两个函数可以同时阻塞多个IO操作**。而且可以同时对多个读操作、写操作的IO函数进行检测。知道有数据可读或可写时，才**真正**调用IO操作函数
5. 异步io，可以**调用aio_read函数告诉内核描述字缓冲区指针和缓冲区的大小、文件偏移及通知的方式**，然后立即返回，当内核将数据拷贝到缓冲区后，再通知应用程序。

---

## epoll面试简答

从用户发起读取数据（read()）开始，陷入内核。

首先创建一个epoll对象

然后使用epoll_ctl对这个对象进行操作（添加、删除、修改），把需要监控的描述符加进去

这些描述符将会以**epoll_event结构体**的形式组成一颗**[红黑树]()（查询的复杂度降为logn了）**

接着阻塞在**epoll_wait**，进入大循环

当某个fd上有事件发生时，内核将会把其对应的结构体放入一个**[链表]()**中，返回有事件发生的[链表]()，用户再遍历这个[链表]()查看具体是哪个事件。

流程如下图：

![img](https://uploadfiles.nowcoder.com/images/20210824/675098158_1629820075166/96BDF76C6EB51C3CAD33E2EAA0F46B8D)

最后，内核空间将数据和事件数量放入（write()）用户空间中。用户再从用户空间中读取数据（read()）



---

## Select、Epoll *

**1.网卡接受数据**

这个过程涉及到DMA传输、IO通路选择等硬件有关的知识，但只需知道：**网卡会把接收到的数据写入内存。**

**2.如何知道接受了数据**

要从**CPU**的角度来看数据接收。

一般而言，由硬件产生的信号需要cpu立马做出回应（不然数据可能就丢失），所以它的优先级很高。cpu理应中断掉正在执行的程序，去做出响应；

当网卡把数据写入到内存后，**网卡向cpu发出一个中断信号，操作系统便能得知有新数据到来**，再通过网卡**中断程序**去处理数据。

**3.进程阻塞为什么不占用Cpu资源**

阻塞是进程调度的关键一环，指的是进程在等待某事件（如接收到网络数据）发生之前的等待状态，也可以说是因为缺乏资源而等待（而不是缺乏CPU使用权）

### **4.阻塞原理**

运行的进程会在计算机的运行队列，处于运行状态，会分时进行。

**需要资源的线程会在等待队列**，等待队列是个非常重要的结构，它指向所有需要等待该socket事件的进程。

当程序缺乏资源，就会从工作队列移动到等待队列，依据进程调度，cpu会轮流执行工作队列其他进程的程序，不会执行阻塞的程序。**所以进程被阻塞，不会往下执行代码，也不会占用cpu资源**。

### 5.唤醒进程

等待队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。

![img](https://pic1.zhimg.com/80/v2-1c7a96c8da16f123388e46f88772e6d8_720w.jpg)

**6.高并发的核心**

高并发的核心方案是一个进程/线程处理所有链接的“等待消息准备好”的状态。

select和eopll都能实现，一个线程处理全部连接，也就是IO多路复用。

**7.异步io**

异步io，用户线程在数据还没准备好的时候既不阻塞也不反复查询，而是 **继续干自己的事情**。

内核会开启一个新线程去读取数据，当数据准备好的时候（内核把数据拷贝到指定缓存区的时候），内核会给用户线程一个信号，产生中断，去执行信号处理函数。

**8.MMAP**

mmap**将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址**（不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理地址），使得这块物理内存对内核和对用户均可见，减少用户态和内核态之间的数据交换。

### Select

![img](https://pic4.zhimg.com/80/v2-0cccb4976f8f2c2f8107f2b3a5bc46b3_720w.jpg)

Select的文件描述符有上限，1024

当有信息到达，那么就需要找出有报文到达的活跃链接是哪一个的时候，会调用select。

**每一次select都是轮询，也就是需要遍历一遍FD_Set列表的全部，才能找到准备好的IO事件。**

套接字存储以一个双向链表存储。

### Epoll

![img](https://pic4.zhimg.com/80/v2-40bd5825e27cf49b7fd9a59dfcbe4d6f_720w.jpg)

Epoll的改进

1. 分离操作，“维护等待队列”用epoll_ctl()，“查询，阻塞线程”用epoll_wait()
2. 自己也是一个文件描述符，也带一个就绪列表，直接把可以使用的socket拷到内核的一张表中，就不需要频繁切换内核态和用户态。
3. 内核和用户空间mmap了同一块内存，内核可以直接看到epoll监听的句柄，效率高。

**核心思想：分清楚频繁调用的操作和不频繁调用的操作，比如等待队列的维护（一开始的全部socket）是不频繁更改的，但是查询是否有信息到达应该是频繁的。**

用红黑树存储所有套接字，并且**添加套接字的时候会跟对应的设备（网卡）驱动程序建立一个回调关系**，实现O1回调/相应。

事件发生，回调，**把事件添加到rdlist就绪列表（双向链表）**，epoll_wait()只需要检查rdlist，而不像select需要检查全部。

这里也需要将发生了的事件复制到用户态内存中即可，但是因为使用了mmap所以并不会切换。

epoll_wait()

1. epoll_wait 调用 ep_poll， 如果rblist空，挂起当前进程（wait的那个进程），**直到rblist不为空才唤醒**。
2. 文件fd状态改变，导致相应的fd的回调ep_poll_callback()被调用
3. ep_poll_callback**除了响应设备程序，还将对应的epitem加入到rdlist**，导致rdlist不空，进程被唤醒，epoll_wait继续执行。
4. 处理完成，ep_events_transfer函数将rdlist的epitem拷到txlist中，清空rdlist。
5. ep_events_transfer扫描txlist的每个epitem，调用关联fd的poll方法。此时的调用**只是为了取得fd较新的events，之后将取得的events和相应的fd发到用户空间(mmap)。**

-----

## 惊群效应 *

https://blog.csdn.net/lyztyycode/article/details/78648798

多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会**唤醒等待的所有进程（或者线程），但是最终却只可能有一个进程（线程）获得这个时间的“控制权”**

消耗了什么

1. 系统对进程、线程频繁地做无效的调度，需要上下文切换
2. 为了确保只有一个线程得到资源，用户必须对资源操作进行加锁保护

**解决：**

1. Nginx解决，ngx_process_events_and_timers()是处理事件的函数
   通过是否对accept加锁来解决惊群问题
2. Linux的**SO_REUSEPORT**特性 解决，该特性支持多个进程或者线程绑定到同一端口，提高服务器程序的性能，**允许多个套接字bind()以及listen()同一个TCP或UDP端口**，并且在内核层面实现负载均衡。
   **每一个线程拥有自己的服务器套接字，在服务器套接字上没有锁的竞争。**

---

## Linux中断调度底层 *

进程切换只发生在内核态。**在进程切换之前，需要通过中断的方式执行系统调用由用户态进入内核态。**

从系统调用或者异常中断返回用户空间时，thread_flags 被设置成TIF_NEED_RESCHED 会发生调度

中断返回用户空间

1. 首先会到 linux-4.10/arch/arm64/kernel/entry.S，开始中断

   1. **用户进程使用的所有寄存器内容都已被SAVE_ALL汇编指令压入内核态堆栈。**

   2. ```c
      #define SAVE_ALL /
          cld; /
          pushl %es; /
          pushl %ds; /
          pushl %eax; /
          pushl %ebp; /
          pushl %edi; /
          pushl %esi; /
          pushl %edx; /
          pushl %ecx; /
          pushl %ebx; /
          movl $(__USER_DS), %edx; /
          movl %edx, %ds; /
          movl %edx, %es;
      ```

2. 如果是crash异常，会进入linux-4.10/arch/arm64/kernel/signal.c

   1. 在这里thread_flags 被设置成（或者添加）**_TIF_NEED_RESCHED**
   2. 调用__schedule()来调度进程

3. 进入linux-4.10/kernel/sched/core.c 

   1. 跑到__schedule() ，参数preempt 表示是否运行抢占

4. __schedule()

   1. pick_next_task() 函数从调度器中获取下一个需要运行的进程（线程）

5. 进入linux-4.10/kernel/sched/sched.h

   1. 判断当前进程是不是 fair_sched_class 调度器（公平调度）
      1. 是，下一进程也使用 fair_sched_class 调度
      2. 不是，尝试使用其他调度器，顺序是stop_sched_class ->dl_sched_class ->rt_sched_class ->fair_sched_class ->idle_sched_class。 

6. 结束，回到linux-4.10/kernel/sched/core.c的__schedule()

   1. 此时已经选择好下一个进程，调度方式
   2. 设置好mm内存地址空间后，就开始调用switch_to()切换进程。 

7. 进入 linux-4.10/include/asm-generic/switch_to.h

   ```c++
   #define switch_to(prev, next, last)					\
   do {								\
   	((last) = __switch_to((prev), (next)));		\
   } while (0)
   ```

   这是重点，这表示 **一次进程切换涉及到三个进程**，很明显prev是当前进程，next是想要的下一个进程。而last则是一个很哲学的设计！

   ```c
   AAA
   switch_to(prev, next, last)
   BBB
   ```

   现在只想要prev切换到next，那么假设有个进程A执行完AAA之后要切换到**另一个代码段的进程B**，A进程调用switch_to()之后cpu就执行B进程了，后续B进程切换到哪对于A来说并不关心！**唯一关心的是当切换回A的时候，cpu执行的上一个task属于哪个进程，也就是这里last进程的哲学**

8. 进入 linux-4.10/arch/arm64/kernel/process.c 

   1.  fpsimd_thread_switch(next); **fp是float-point的意思**，和浮点运算相关。**simd是Single Instruction Multiple Data的意思**，和多媒体以及信号处理相关。
       fpsimd_thread_switch其实就是**把当前FPSIMD的状态保存到了内存中（task.thread.fpsimd_state）**，从要切入的next进程描述符中获取FPSIMD状态，并加载到CPU上。
   2.  tls_thread_switch(next); **tls（thread local storage）的切换。**
   3.  hw_breakpoint_thread_switch(next); **断点处的进程切换**
   4.  contextidr_thread_switch(next); **上下文切换**
   5.  entry_task_switch(next); **程序进入点切换**
   6.  调用cpu_switch_to()进行cpu状态的切换。 

9. 进入linux-4.10/arch/arm64/kernel/entry.S（**汇编处理**）
   cup context的切换

   1. prev 寄存器中的状态保存到内存中，保存在prev->thread.cpu_context 中
   2. next 之前的状态（next->thread.cpu_context）恢复到对应的寄存器。 

10. 进入 linux-4.10/include/linux/sched.h

11. 进入 linux-4.10/arch/arm64/include/asm/processor.h 

12. 进入 linux-4.10/arch/arm64/include/asm/processor.h 读取此操作系统中某些寄存器的地址值

13. 完成切换后，会调用finish_task_switch()函数做一些清尾工作。
    进入 linux-4.10/kernel/sched/core.c

    1. mm释放，考虑到内核线程使用用户线程的mm情况
    2. 如果上一个进程已经终止，释放其task_struct 结构

补充：https://www.cnblogs.com/yudao/p/4388575.html

https://www.cnblogs.com/wk2016just/p/6718487.html

---

## 堆栈区别

**通常情况下由操作系统（OS）和语言的运行时（runtime）控制吗？**

1. 栈是为执行线程留出的内存空间。当函数被调用的时候，**栈顶为局部变量和一些 bookkeeping 数据预留块。**当函数执行完毕，块就没有用了，可能在下次的函数调用的时候再被使用。
   在栈上的每个字节频繁的被复用也就意味着它可能映射到处理器缓存中，所以很快**（局部性原理）**。
2. 堆（heap）是为动态分配预留的内存空间。通常情况下，操作系统通过**调用语言的运行时（runtime）机制**去为应用程序分配堆。 
   然而堆在分配和释放的时候有**更多的复杂的 bookkeeping 参与。**

**存储方式？**

1. 栈的存储内容在物理上也是连续的
   1. 函数调用时要进栈**函数来源处的下一条指令**的地址
   2. 函数调用时需要把参数压栈
      1. __stdcall：windows api默认，从右往左
      2. __cdecl：C/C++默认的函数调用协议，从右往左
      3. __fastcall：从左开始不大于4字节的参数放入CPU的ECX和EDX寄存器，其余参数从右向左入栈。
2. 堆的头部用一个字节存放堆的大小，其余具体内容是离散的由程序员安排

**大小？**

依赖于语言，编译器，操作系统和架构。

栈通常提前分配好了，因为栈必须是连续的内存块。语言的编译器或者操作系统决定它的大小。**默认1m**

堆从低地址向高地址增长，在Window下，栈的大小一般小于2GB。

**堆的大小受限于计算机系统中有效的虚拟内存。一般来讲在32位系统下，堆内存可以达到2.9G的大小。（除去1G的内核空间，几乎占满3G的用户空间）**

**系统的响应？**

栈：申请空间小于已有空间就分配，否则报 **栈溢出crash**

堆：收到程序的申请，遍历记录空闲内存地址的链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲链表中删除，并将该结点的空间分配给程序。

**速度？**	

这个答案是依赖于实现的

栈：快，计算机会在底层对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行

堆：C/C++函数库提供分配方法，编译器去申请空间，它的机制是很复杂的

---

